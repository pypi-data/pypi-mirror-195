import re
from typing import Any, Dict, List, Text,Optional
import pyarabic.araby as araby
import json
import time
from itertools import chain 
import datefinder 
import jellyfish  #correctionPackage"
import unicodedata 
from pyarabic.unshape import unshaping_line
import arabic_reshaper
from custom_func.clean_data_ar import clean_data_ar 
from custom_func.clean_data_ar_training import clean_data_ar_training
from itertools import *
from dateutil.parser import parse
from rasa.nlu.components import Component
from rasa.nlu.config import RasaNLUModelConfig
from rasa.nlu.tokenizers.tokenizer_fr import Token, Tokenizer
from rasa.nlu.training_data import Message, TrainingData
from custom_func.translator_darijaFr_DarijaAr import trans 
from custom_func.generateur_oncf_ar import generateur_oncf_ar
from rasa.nlu.constants import (
    TOKENS_NAMES,
    MESSAGE_ATTRIBUTES,
    TEXT,
    INTENT,
)

class WhitespaceTokenizer_oncf_ar_v2(Tokenizer, Component):

    provides = [TOKENS_NAMES[attribute] for attribute in MESSAGE_ATTRIBUTES]


    def unique_words(lines):
        return set(chain(*(line.split() for line in lines if line)))
    
    dict= {}

    
    defaults = {
        # text will be tokenized with case sensitive as default
        "case_sensitive": True,
        "token_pattern": None,
    }

    def __init__(self, component_config: Dict[Text, Any] = None) -> None:
        """Construct a new tokenizer using the WhitespaceTokenizer framework."""

        super(WhitespaceTokenizer_oncf_ar_v2, self).__init__(component_config)

        self.case_sensitive = self.component_config["case_sensitive"]

    def train(
        self,
        training_data: TrainingData,
        config: Optional[RasaNLUModelConfig] = None,
        **kwargs: Any,
    ) -> None:
        generateur_oncf_ar('data_oncf_ar_v3',)
        """Tokenize all training data."""

        for example in training_data.training_examples:
            for attribute in MESSAGE_ATTRIBUTES:
                if example.get(attribute) is not None:
                    if attribute == INTENT:
                        tokens = self._split_intent(example)
                    else:
                        tokens = self.tokenize(example, attribute)
                        tokens = self.add_cls_token(tokens, attribute)
                    example.set(TOKENS_NAMES[attribute], tokens)

    def process(self, message: Message, **kwargs: Any) -> None:
        lettre="abcdefghijklmnopqrstuvwxyzABSCDEFGHIJKLMNOPQRSTUVWXYZ123456789"
        nbre = "123546879"
        arabe="ابتثحخدذجرزسشصضطظعغفقكمنهويءإأٱآةؤئىل"
        txt = message.text
        texte = txt.split(" ")
         
        
        print(texte)
 
        
        for i in texte:
            
            numb = 0 
            if i.isdigit() :
                numb = 0
                continue
            else :
                
                for l in i :
                    if l in arabe :
                        
                        tokens = self.tokenize_arab(message, TEXT)
                        numb = 1
                        break
                    elif l== "\u200f" :
                        
                        continue
                    else :
                        
                        tokens = self.tokenize_dr(message, TEXT)
                        numb = 1
                        break
            break
        if numb == 0 :
            tokens = self.tokenize_arab(message, TEXT)
            
        
        tokens = self.add_cls_token(tokens, TEXT)
        message.set(TOKENS_NAMES[TEXT], tokens)
                     

    def tokenize(self, message: Message, attribute: Text) -> List[Token]:
        text = message.get(attribute)

        
        text = text.lower()
        #Remove punctuation !!

        print( text)
        new_text = ''
        punct='''!()-[]{};:"\,<>./?#$%^&*_~'''
        for t in text :
            if t not in punct :
                
                new_text = new_text  +  t
            else :
                new_text = new_text + " "
        print(new_text)
        
        text = new_text
        

        # remove 'not a word character' if
        words = re.sub(
            # there is a space or an end of a string after it
            r"[^\w#@&]+(?=\s|$)|"
            # there is a space or beginning of a string before it
            # not followed by a number
            r"(\s|^)[^\w#@&]+(?=[^0-9\s])|"
            # not in between numbers and not . or @ or & or - or #
            # e.g. 10'000.00 or blabla@gmail.com
            # and not url characters
            r"(?<=[^0-9\s])[^\w._~:/?#\[\]()@!$&*+,;=-]+(?=[^0-9\s])",
            " ",
            text,
        ).split()
        with open("dictionaries/dict_oncf_ar.txt", 'r',encoding="utf8") as f:
            list2 = set(chain(*(line.split() for line in f if line)))
        wordss = []
        words_2 = []
        wordss = clean_data_ar_training(list2,words)
        # if we removed everything like smiles `:)`, use the whole text as 1 token
        if not wordss:
            wordss = [text]
        
        tokens = []
        running_offset = 0
        # if we removed everything like smiles `:)`, use the whole text as 1 token
        stop_words = ['ال','الله','ي','علا','كان','شهي','هل','باغي','و','ف','ن','لس','ف','نسولك','عافاك','ممكن','مومكين','في','ف','شي','تقدر','بغيت', 'بغهيت','ديال','ديالكم','واش','ل','نقدر','لس','لك','لو','س','في','لديكم','ف','الا','ديالي','ا','ل','كاين','ولا','يلا']
        for word in wordss:
            
            word_offset = text.index(word, running_offset)
            word_len = len(word)
            running_offset = word_offset + word_len
            tokens.append(Token(word, word_offset))

        return tokens


    def tokenize_dr(self, message: Message, attribute: Text ) -> List[Token]:
        text = message.get(attribute)
        
        text = text.lower()
        
        numb = ""
        date = ""
        hour = ""
        
        string = text
        
        new_text = ''
        punct='''!()-[]{};:'"\,<>./?#$%^&*_~'''
        for t in text :
            if t not in punct :
                
                new_text = new_text  +  t
            else :
                new_text = new_text + " "
        
        text = new_text
#replace indou arabic numbers
        if '۰' in string :
		        string = string.replace('۰','0')
        if '١' in string :
            string = string.replace('١','1')
        if '٢' in string :
            string = string.replace('٢','2')
        if '۳' in string :
            string = string.replace('۳','3')
        if '۴' in string :
            string = string.replace('۴','4')
        if '۵' in string :
            string = string.replace('۵','5')
        if '۶' in string :
            string = string.replace('۶','6')
        if '۷' in string :
            string = string.replace('۷','7')
        if '۸' in string :
            string = string.replace('۸','8')
        if '۹' in string :
            string = string.replace('۹','9')

        text = string

        for s in text.split(" "):
            if s.isnumeric():
                numb = s
            dates = re.search(r'(\d{1,2}([.\-/])\d{1,2})',s)
            if dates == None :
                pass
            else : 
                date = dates.string
            hours1 = re.search(r'(([0-9]+)+(h|heure|heures))',s)
            
            if hours1 == None :
                hours1 = re.search(r'(([0-9]+)+(h|heure|heures)).([0-9]+)',s)
            if hours1 == None :
                hours1 = re.search(r'(([0-9]+):([0-9]+))',s)
            if hours1 == None:
                pass
            else :
                
                hour = hours1.string 
        new_text= ''
        hours1 = re.search(r'(([0-9]+)+(h|heure|heures))',s)
        hours2 = re.search(r'(([0-9]+)+(h|heure|heures)).([0-9]+)',s)
        hours3 = re.search(r'(([0-9]+):([0-9]+))',s)
        
        for s in text.split(" "):
            
            dates = re.search(r'(\d{1,2}([.\-/])\d{1,2})',s)
            
            if s.isdigit():
                new_text = new_text + ' ' + s
                print('numeric'+new_text)
            
            elif dates != None :
                pass
                new_text = new_text + ' ' + s
            elif hours1 != None :
                pass
                new_text = new_text + ' ' + s
            elif hours2 != None :
                
                new_text = new_text + ' ' + s
            elif hours3 != None :
                
                new_text = new_text + ' ' + s
            
            else : 
                new_text = new_text + ' ' + trans(s)
                

        words = re.sub(
            # there is a space or an end of a string after it
            r"[^\w#@&]+(?=\s|$)|"
            # there is a space or beginning of a string before it
            # not followed by a number
            r"(\s|^)[^\w#@&]+(?=[^0-9\s])|"
            # not in between numbers and not . or @ or & or - or #
            # e.g. 10'000.00 or blabla@gmail.com
            # and not url characters
            r"(?<=[^0-9\s])[^\w._~:/?#\[\]()@!$&*+,;=-]+(?=[^0-9\s])",
            " ",
            new_text,
        ).split()
        
        running_offset = 0
        tokens = []
        dicto={}
        texte = ''
        words_corr = []
        word_cor = ''
        with open("dictionaries/dict_oncf_ar.txt", 'r',encoding="utf8") as f:
            list1 = set(chain(*(line.split() for line in f if line)))
        
        stop_words = ['ال','الله','ي','علا','كان','شهي','هل','باغي','و','ف','ن','لس','ف','نسولك','عافاك','ممكن','مومكين','في','ف','شي','تقدر','بغيت', 'بغهيت','ديال','ديالكم','واش','ل','نقدر','لس','لك','لو','س','في','لديكم','ف','الا','ديالي','ا','ل','كاين','ولا','يلا']
        
        
        words2 = []
        words2 = clean_data_ar(list1,words)
        for word in words2:
            
            stop_words = ['ال','الله','ي','علا','كان','شهي','هل','باغي','و','ف','ن','لس','ف','نسولك','عافاك','ممكن','مومكين','في','ف','شي','تقدر','بغيت', 'بغهيت','ديال','ديالكم','واش','ل','نقدر','لس','لك','لو','س','في','لديكم','ف','الا','ديالي','ا','ل','كاين','ولا','يلا']                                   
            if word.isnumeric() or hours1 != None or hours2 != None or hours3 != None or dates != None:
                min_dist = word 
                
            
            else :
                for mot in list1:
                    
                    dicto[mot]=jellyfish.damerau_levenshtein_distance(mot, word)
                min_dist = min(dicto.keys(), key=(lambda k: dicto[k]))
            
            if min_dist != '':
                if jellyfish.damerau_levenshtein_distance(min_dist, word) > 2:
                   min_dist = word
          
            
            if min_dist == '' :
                
                word_cor = 'hjfhjqf'
            else :
                word_cor = min_dist

            words_corr.append(word_cor)
            #print(words_corr+"word_corr")
            texte = texte +" "+word_cor
            
        
        if words_corr == []:
            words_corr.append('hjfhjqf')
            texte = texte + "hjfhjqf"


        for word in words_corr :
            
            word_offset = texte.index(word, running_offset)
            word_len = len(word)
            running_offset = word_offset + word_len
            tokens.append(Token(word, word_offset))
            
        
        return tokens

    def tokenize_arab(self, message: Message, attribute: Text) -> List[Token]:
        text = message.get(attribute)
        if not self.case_sensitive:
            text = text.lower()
        
        new_text = ''
        punct='''!()-[]{};:'"\,<>./?#$%^&*_~'''
        for t in text :
            if t not in punct :
                
                new_text = new_text  +  t
            else :
                new_text = new_text + " "
        
        text = new_text
        # remove 'not a word character' if
        numb = ""
        date1 = ""
        date2 = ""
        hour = ""
        words = re.sub(
            # there is a space or an end of a string after it
            r"[^\w#@&]+(?=\s|$)|"
            # there is a space or beginning of a string before it
            # not followed by a number
            r"(\s|^)[^\w#@&]+(?=[^0-9\s])|"
            # not in between numbers and not . or @ or & or - or #
            # e.g. 10'000.00 or blabla@gmail.com
            # and not url characters
            r"(?<=[^0-9\s])[^\w._~:/?#\[\]()@!$&*+,;=-]+(?=[^0-9\s])",
            " ",
            text,
        ).split()
        #print(words)
        running_offset = 0
        tokens = []
        with open("dictionaries/dict_oncf_ar.txt", 'r',encoding="utf8") as f:
             list1 = set(chain(*(line.split() for line in f if line)))
        dicto= {}
        for s in text.split():
            if s.isnumeric():
                numb = s
                
                date1 = re.search(r'(\d{1,2}([/])\d{1,2}([/])\d{1,4})',s)
                
                if date1 == None :
                    pass
                else :
                    date2 = date1.string
                hours = re.search(r'([0-9]+(h))',s) 
                if hours == None :
                    pass
                else :
                    hour = hours.string

        text = text +" "+ date2 + "  " + numb + " " + hour
        stop_words = ["ال",'الله','ي','علا','كان','شهي','هل','باغي','و','ف','ن','لس','ف','نسولك','عافاك','ممكن','مومكين','في','ف','شي','تقدر','بغيت', 'بغهيت','ديال','ديالكم','واش','ل','نقدر','لس','لك','لو','س','في','لديكم','ف','الا','ديالي','ا','ل','كاين','ولا','يلا']
        
        words2 = clean_data_ar(list1,words)
        
        texte = ""
        for w in words2 :
            texte = texte + w + " "
        if words2 == []:
            words2.append("hjfhjqf")
            texte = texte + "hjfhjqf"
        for word in words2:
            if word.isnumeric() :
                min_dist = word
            
            else :
                for mot in list1:
                    dicto[mot]=jellyfish.damerau_levenshtein_distance(mot, word)
                min_dist = min(dicto.keys(), key=(lambda k: dicto[k]))
            
            if min_dist != '':
                if jellyfish.damerau_levenshtein_distance(min_dist, word) > 2:
                   min_dist = word


            if min_dist == '' :
                
                word_corr = 'hjfhjqf'
            else :
                word_corr = min_dist
            
            word_offset = texte.index(word, running_offset)
            word_len = len(word)
            running_offset = word_offset + word_len
            tokens.append(Token(word_corr, word_offset))
        
        return tokens

    